#!/usr/bin/env python3
"""
Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡ Ù„Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù†
Performance Test Suite for Optimized System
"""

import asyncio
import logging
import time
import random
import statistics
from typing import Dict, List, Tuple
import matplotlib.pyplot as plt
import numpy as np
from concurrent.futures import ThreadPoolExecutor
import psutil
import gc

from advanced_queue_system import AdvancedQueueSystem, MediaTask, TaskPriority
from parallel_file_processor import ParallelFileProcessor
from optimized_media_handler import OptimizedMediaHandler

logger = logging.getLogger(__name__)

class PerformanceTestSuite:
    """Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡"""
    
    def __init__(self):
        """ØªÙ‡ÙŠØ¦Ø© Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª"""
        self.results = {
            'queue_system': {},
            'file_processor': {},
            'media_handler': {},
            'system_resources': {}
        }
        
        # Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ø®ØªØ¨Ø§Ø± Ù…ØªÙ†ÙˆØ¹Ø©
        self.test_files = self._generate_test_files()
        
    def _generate_test_files(self) -> List[Dict]:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„ÙØ§Øª Ø§Ø®ØªØ¨Ø§Ø± Ù…ØªÙ†ÙˆØ¹Ø©"""
        files = []
        
        # Ù…Ù„ÙØ§Øª ØµØºÙŠØ±Ø© (1-5 MB)
        for i in range(10):
            size = random.randint(1024*1024, 5*1024*1024)  # 1-5 MB
            files.append({
                'name': f'small_file_{i}.jpg',
                'size': size,
                'data': b'0' * size,
                'type': 'image'
            })
        
        # Ù…Ù„ÙØ§Øª Ù…ØªÙˆØ³Ø·Ø© (10-50 MB)
        for i in range(5):
            size = random.randint(10*1024*1024, 50*1024*1024)  # 10-50 MB
            files.append({
                'name': f'medium_file_{i}.mp4',
                'size': size,
                'data': b'0' * size,
                'type': 'video'
            })
        
        # Ù…Ù„ÙØ§Øª ÙƒØ¨ÙŠØ±Ø© (100-200 MB)
        for i in range(3):
            size = random.randint(100*1024*1024, 200*1024*1024)  # 100-200 MB
            files.append({
                'name': f'large_file_{i}.mp4',
                'size': size,
                'data': b'0' * size,
                'type': 'video'
            })
        
        return files
    
    async def test_queue_system_performance(self) -> Dict:
        """Ø§Ø®ØªØ¨Ø§Ø± Ø£Ø¯Ø§Ø¡ Ù†Ø¸Ø§Ù… Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø±"""
        logger.info("ğŸ§ª Ø¨Ø¯Ø¡ Ø§Ø®ØªØ¨Ø§Ø± Ø£Ø¯Ø§Ø¡ Ù†Ø¸Ø§Ù… Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø±...")
        
        queue_system = AdvancedQueueSystem(max_workers=8)
        
        # Ø§Ø®ØªØ¨Ø§Ø± Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ù‡Ø§Ù…
        start_time = time.time()
        task_ids = []
        
        for file_data in self.test_files[:10]:  # Ø§Ø®ØªØ¨Ø§Ø± Ø£ÙˆÙ„ 10 Ù…Ù„ÙØ§Øª
            task = MediaTask(
                media_data=file_data['data'],
                filename=file_data['name'],
                file_size=file_data['size'],
                media_type=file_data['type'],
                processing_type='watermark',
                priority=random.choice(list(TaskPriority))
            )
            
            task_id = queue_system.add_task(task)
            task_ids.append(task_id)
        
        add_tasks_time = time.time() - start_time
        
        # Ø§Ù†ØªØ¸Ø§Ø± Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„Ù…Ù‡Ø§Ù…
        start_processing = time.time()
        completed_tasks = 0
        
        while completed_tasks < len(task_ids):
            await asyncio.sleep(0.1)
            completed_tasks = queue_system.stats['completed_tasks'] + queue_system.stats['failed_tasks']
        
        processing_time = time.time() - start_processing
        
        # Ø¬Ù…Ø¹ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        stats = queue_system.get_queue_stats()
        
        queue_system.shutdown()
        
        results = {
            'add_tasks_time': add_tasks_time,
            'processing_time': processing_time,
            'total_time': add_tasks_time + processing_time,
            'tasks_per_second': len(task_ids) / (add_tasks_time + processing_time),
            'stats': stats,
            'throughput_mbps': sum(f['size'] for f in self.test_files[:10]) / (1024*1024) / processing_time
        }
        
        self.results['queue_system'] = results
        logger.info(f"âœ… Ø§Ø®ØªØ¨Ø§Ø± Ù†Ø¸Ø§Ù… Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ù…ÙƒØªÙ…Ù„: {results['tasks_per_second']:.2f} Ù…Ù‡Ù…Ø©/Ø«Ø§Ù†ÙŠØ©")
        
        return results
    
    async def test_file_processor_performance(self) -> Dict:
        """Ø§Ø®ØªØ¨Ø§Ø± Ø£Ø¯Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù…Ù„ÙØ§Øª"""
        logger.info("ğŸ§ª Ø¨Ø¯Ø¡ Ø§Ø®ØªØ¨Ø§Ø± Ø£Ø¯Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù…Ù„ÙØ§Øª...")
        
        processor = ParallelFileProcessor(max_workers=6, chunk_size=10*1024*1024)
        
        # Ø§Ø®ØªØ¨Ø§Ø± ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù…Ù„ÙØ§Øª
        chunking_times = []
        upload_times = []
        
        def dummy_upload(data: bytes) -> bool:
            # Ù…Ø­Ø§ÙƒØ§Ø© Ø±ÙØ¹ Ø¨Ø·ÙŠØ¡
            time.sleep(random.uniform(0.1, 0.3))
            return True
        
        for file_data in self.test_files[10:15]:  # Ø§Ø®ØªØ¨Ø§Ø± Ù…Ù„ÙØ§Øª Ù…ØªÙˆØ³Ø·Ø©
            # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªÙ‚Ø³ÙŠÙ…
            start_chunk = time.time()
            chunks = processor.create_file_chunks(file_data['data'], file_data['name'])
            chunking_time = time.time() - start_chunk
            chunking_times.append(chunking_time)
            
            # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø±ÙØ¹ Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ
            start_upload = time.time()
            results = processor.parallel_upload_chunks(chunks, dummy_upload)
            upload_time = time.time() - start_upload
            upload_times.append(upload_time)
        
        processor.shutdown()
        
        results = {
            'average_chunking_time': statistics.mean(chunking_times),
            'average_upload_time': statistics.mean(upload_times),
            'chunking_speed_mbps': statistics.mean([
                (f['size'] / (1024*1024)) / t for f, t in 
                zip(self.test_files[10:15], chunking_times)
            ]),
            'upload_speed_mbps': statistics.mean([
                (f['size'] / (1024*1024)) / t for f, t in 
                zip(self.test_files[10:15], upload_times)
            ])
        }
        
        self.results['file_processor'] = results
        logger.info(f"âœ… Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù…Ù„ÙØ§Øª Ù…ÙƒØªÙ…Ù„: {results['upload_speed_mbps']:.2f} MB/s")
        
        return results
    
    def test_system_resources(self) -> Dict:
        """Ø§Ø®ØªØ¨Ø§Ø± Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ù†Ø¸Ø§Ù…"""
        logger.info("ğŸ§ª Ø¨Ø¯Ø¡ Ø§Ø®ØªØ¨Ø§Ø± Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ù†Ø¸Ø§Ù…...")
        
        # Ù‚ÙŠØ§Ø³ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ù‚Ø¨Ù„ Ø§Ù„ØªØ´ØºÙŠÙ„
        initial_memory = psutil.virtual_memory().used
        initial_cpu = psutil.cpu_percent(interval=1)
        
        # ØªØ´ØºÙŠÙ„ Ø§Ø®ØªØ¨Ø§Ø± Ù…ÙƒØ«Ù
        queue_system = AdvancedQueueSystem(max_workers=16)
        processor = ParallelFileProcessor(max_workers=8)
        
        # Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ´ØºÙŠÙ„
        memory_usage = []
        cpu_usage = []
        
        async def monitor_resources():
            for _ in range(30):  # Ù…Ø±Ø§Ù‚Ø¨Ø© Ù„Ù…Ø¯Ø© 30 Ø«Ø§Ù†ÙŠØ©
                memory_usage.append(psutil.virtual_memory().used)
                cpu_usage.append(psutil.cpu_percent())
                await asyncio.sleep(1)
        
        # ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ù‡Ø§Ù… Ù…Ø¹ Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©
        async def run_intensive_tasks():
            tasks = []
            for file_data in self.test_files:
                task = MediaTask(
                    media_data=file_data['data'][:1024*1024],  # ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø­Ø¬Ù… Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
                    filename=file_data['name'],
                    file_size=1024*1024,
                    media_type=file_data['type'],
                    processing_type='watermark'
                )
                queue_system.add_task(task)
            
            # Ø§Ù†ØªØ¸Ø§Ø± Ù„ÙØªØ±Ø©
            await asyncio.sleep(30)
        
        # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± ÙˆØ§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© Ù…Ø¹Ø§Ù‹
        async def run_test():
            await asyncio.gather(
                monitor_resources(),
                run_intensive_tasks()
            )
        
        asyncio.run(run_test())
        
        # ØªÙ†Ø¸ÙŠÙ
        queue_system.shutdown()
        processor.shutdown()
        gc.collect()
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        max_memory = max(memory_usage)
        avg_memory = statistics.mean(memory_usage)
        max_cpu = max(cpu_usage)
        avg_cpu = statistics.mean(cpu_usage)
        
        results = {
            'initial_memory_mb': initial_memory / (1024*1024),
            'max_memory_mb': max_memory / (1024*1024),
            'avg_memory_mb': avg_memory / (1024*1024),
            'memory_increase_mb': (max_memory - initial_memory) / (1024*1024),
            'initial_cpu_percent': initial_cpu,
            'max_cpu_percent': max_cpu,
            'avg_cpu_percent': avg_cpu,
            'memory_timeline': [m / (1024*1024) for m in memory_usage],
            'cpu_timeline': cpu_usage
        }
        
        self.results['system_resources'] = results
        logger.info(f"âœ… Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ù…ÙƒØªÙ…Ù„: Ø°Ø±ÙˆØ© Ø§Ù„Ø°Ø§ÙƒØ±Ø© {results['memory_increase_mb']:.1f}MB")
        
        return results
    
    def benchmark_vs_original(self) -> Dict:
        """Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ Ù…Ø¹ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø£ØµÙ„ÙŠ"""
        logger.info("ğŸ§ª Ø¨Ø¯Ø¡ Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡...")
        
        # Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø£ØµÙ„ÙŠ (Ù…ØªØ³Ù„Ø³Ù„)
        def original_system_simulation():
            start_time = time.time()
            
            for file_data in self.test_files[:5]:
                # Ù…Ø­Ø§ÙƒØ§Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªØ³Ù„Ø³Ù„Ø©
                processing_time = file_data['size'] / (10*1024*1024)  # 10MB/s
                time.sleep(processing_time)
            
            return time.time() - start_time
        
        # Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù†
        async def optimized_system_test():
            start_time = time.time()
            queue_system = AdvancedQueueSystem(max_workers=8)
            
            for file_data in self.test_files[:5]:
                task = MediaTask(
                    media_data=file_data['data'][:1024*1024],  # ØªÙ‚Ù„ÙŠÙ„ Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
                    filename=file_data['name'],
                    file_size=1024*1024,
                    media_type=file_data['type'],
                    processing_type='watermark'
                )
                queue_system.add_task(task)
            
            # Ø§Ù†ØªØ¸Ø§Ø± Ø§Ù„Ø¥ÙƒÙ…Ø§Ù„
            while queue_system.stats['completed_tasks'] + queue_system.stats['failed_tasks'] < 5:
                await asyncio.sleep(0.1)
            
            queue_system.shutdown()
            return time.time() - start_time
        
        # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª
        original_time = original_system_simulation()
        optimized_time = asyncio.run(optimized_system_test())
        
        improvement_factor = original_time / optimized_time if optimized_time > 0 else 0
        
        results = {
            'original_time': original_time,
            'optimized_time': optimized_time,
            'improvement_factor': improvement_factor,
            'time_saved_percent': ((original_time - optimized_time) / original_time) * 100
        }
        
        logger.info(f"âœ… Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡: ØªØ­Ø³Ù† Ø¨Ù†Ø³Ø¨Ø© {improvement_factor:.1f}x")
        
        return results
    
    def generate_performance_report(self) -> str:
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ø£Ø¯Ø§Ø¡ Ø´Ø§Ù…Ù„"""
        report = """
# ğŸ“Š ØªÙ‚Ø±ÙŠØ± Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù†

## ğŸš€ Ù…Ù„Ø®Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬

### Ù†Ø¸Ø§Ù… Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± (Queue System):
- Ø³Ø±Ø¹Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {queue_tasks_per_sec:.2f} Ù…Ù‡Ù…Ø©/Ø«Ø§Ù†ÙŠØ©
- Ø¥Ù†ØªØ§Ø¬ÙŠØ© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {queue_throughput:.2f} MB/s
- ÙˆÙ‚Øª Ø§Ù„Ø¥Ø¶Ø§ÙØ©: {queue_add_time:.3f} Ø«Ø§Ù†ÙŠØ©
- ÙˆÙ‚Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {queue_process_time:.2f} Ø«Ø§Ù†ÙŠØ©

### Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù…Ù„ÙØ§Øª (File Processor):
- Ø³Ø±Ø¹Ø© Ø§Ù„ØªÙ‚Ø³ÙŠÙ…: {file_chunk_speed:.2f} MB/s
- Ø³Ø±Ø¹Ø© Ø§Ù„Ø±ÙØ¹: {file_upload_speed:.2f} MB/s
- Ù…ØªÙˆØ³Ø· ÙˆÙ‚Øª Ø§Ù„ØªÙ‚Ø³ÙŠÙ…: {file_chunk_time:.3f} Ø«Ø§Ù†ÙŠØ©
- Ù…ØªÙˆØ³Ø· ÙˆÙ‚Øª Ø§Ù„Ø±ÙØ¹: {file_upload_time:.2f} Ø«Ø§Ù†ÙŠØ©

### Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ÙˆØ§Ø±Ø¯:
- Ø°Ø±ÙˆØ© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø©: {max_memory:.1f} MB
- Ù…ØªÙˆØ³Ø· Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬: {avg_cpu:.1f}%
- Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø©: {memory_increase:.1f} MB

## ğŸ“ˆ Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡:
- ØªØ­Ø³Ù† Ø§Ù„Ø³Ø±Ø¹Ø©: {improvement:.1f}x Ø£Ø³Ø±Ø¹
- ØªÙˆÙÙŠØ± Ø§Ù„ÙˆÙ‚Øª: {time_saved:.1f}%

## ğŸ¯ Ø§Ù„ØªÙˆØµÙŠØ§Øª:
1. Ø§Ù„Ù†Ø¸Ø§Ù… ÙŠØ¸Ù‡Ø± ØªØ­Ø³Ù†Ø§Ù‹ Ù…Ù„Ø­ÙˆØ¸Ø§Ù‹ ÙÙŠ Ø§Ù„Ø³Ø±Ø¹Ø©
2. Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø¶Ù…Ù† Ø§Ù„Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù…Ù‚Ø¨ÙˆÙ„
3. ÙŠÙÙ†ØµØ­ Ø¨ØªÙØ¹ÙŠÙ„ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù† ÙÙŠ Ø§Ù„Ø¥Ù†ØªØ§Ø¬

""".format(
            queue_tasks_per_sec=self.results['queue_system'].get('tasks_per_second', 0),
            queue_throughput=self.results['queue_system'].get('throughput_mbps', 0),
            queue_add_time=self.results['queue_system'].get('add_tasks_time', 0),
            queue_process_time=self.results['queue_system'].get('processing_time', 0),
            
            file_chunk_speed=self.results['file_processor'].get('chunking_speed_mbps', 0),
            file_upload_speed=self.results['file_processor'].get('upload_speed_mbps', 0),
            file_chunk_time=self.results['file_processor'].get('average_chunking_time', 0),
            file_upload_time=self.results['file_processor'].get('average_upload_time', 0),
            
            max_memory=self.results['system_resources'].get('max_memory_mb', 0),
            avg_cpu=self.results['system_resources'].get('avg_cpu_percent', 0),
            memory_increase=self.results['system_resources'].get('memory_increase_mb', 0),
            
            improvement=self.results.get('benchmark', {}).get('improvement_factor', 0),
            time_saved=self.results.get('benchmark', {}).get('time_saved_percent', 0)
        )
        
        return report
    
    def plot_performance_graphs(self):
        """Ø±Ø³Ù… Ø§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ© Ù„Ù„Ø£Ø¯Ø§Ø¡"""
        try:
            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
            
            # Ø±Ø³Ù… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø©
            if 'memory_timeline' in self.results['system_resources']:
                ax1.plot(self.results['system_resources']['memory_timeline'])
                ax1.set_title('Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¹Ø¨Ø± Ø§Ù„ÙˆÙ‚Øª')
                ax1.set_ylabel('Ø§Ù„Ø°Ø§ÙƒØ±Ø© (MB)')
                ax1.set_xlabel('Ø§Ù„ÙˆÙ‚Øª (Ø«Ø§Ù†ÙŠØ©)')
            
            # Ø±Ø³Ù… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬
            if 'cpu_timeline' in self.results['system_resources']:
                ax2.plot(self.results['system_resources']['cpu_timeline'])
                ax2.set_title('Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬ Ø¹Ø¨Ø± Ø§Ù„ÙˆÙ‚Øª')
                ax2.set_ylabel('Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬ (%)')
                ax2.set_xlabel('Ø§Ù„ÙˆÙ‚Øª (Ø«Ø§Ù†ÙŠØ©)')
            
            # Ø±Ø³Ù… Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø³Ø±Ø¹Ø©
            if 'benchmark' in self.results:
                categories = ['Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø£ØµÙ„ÙŠ', 'Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù†']
                times = [
                    self.results['benchmark'].get('original_time', 0),
                    self.results['benchmark'].get('optimized_time', 0)
                ]
                ax3.bar(categories, times)
                ax3.set_title('Ù…Ù‚Ø§Ø±Ù†Ø© Ø£ÙˆÙ‚Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©')
                ax3.set_ylabel('Ø§Ù„ÙˆÙ‚Øª (Ø«Ø§Ù†ÙŠØ©)')
            
            # Ø±Ø³Ù… Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø·Ø§Ø¨ÙˆØ±
            if 'queue_system' in self.results:
                stats = self.results['queue_system'].get('stats', {}).get('stats', {})
                labels = ['Ù…ÙƒØªÙ…Ù„Ø©', 'ÙØ§Ø´Ù„Ø©', 'Ù†Ø´Ø·Ø©']
                sizes = [
                    stats.get('completed_tasks', 0),
                    stats.get('failed_tasks', 0),
                    stats.get('active_tasks', 0)
                ]
                ax4.pie(sizes, labels=labels, autopct='%1.1f%%')
                ax4.set_title('ØªÙˆØ²ÙŠØ¹ Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ù‡Ø§Ù…')
            
            plt.tight_layout()
            plt.savefig('/workspace/performance_report.png', dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info("ØªÙ… Ø­ÙØ¸ Ø§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ© ÙÙŠ performance_report.png")
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø±Ø³Ù… Ø§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ©: {e}")
    
    async def run_full_test_suite(self) -> Dict:
        """ØªØ´ØºÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„ÙƒØ§Ù…Ù„Ø©"""
        logger.info("ğŸ§ª Ø¨Ø¯Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„ÙƒØ§Ù…Ù„Ø©...")
        
        # ØªØ´ØºÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª
        await self.test_queue_system_performance()
        await self.test_file_processor_performance()
        self.test_system_resources()
        self.results['benchmark'] = self.benchmark_vs_original()
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        report = self.generate_performance_report()
        
        # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        with open('/workspace/performance_report.md', 'w', encoding='utf-8') as f:
            f.write(report)
        
        # Ø±Ø³Ù… Ø§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ©
        self.plot_performance_graphs()
        
        logger.info("âœ… Ø§ÙƒØªÙ…Ù„Øª Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø¨Ù†Ø¬Ø§Ø­")
        
        return self.results

# Ù…Ø«Ø§Ù„ Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
async def main():
    test_suite = PerformanceTestSuite()
    results = await test_suite.run_full_test_suite()
    print("ØªÙ… Ø¥ÙƒÙ…Ø§Ù„ Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡!")

if __name__ == "__main__":
    asyncio.run(main())